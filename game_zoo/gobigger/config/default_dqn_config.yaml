var1: &pipeline 'serial_dqn'
common:
  experiment_name: gb_serial_dqn
  seed: 0
env:
  name: 'gobigger'
  team_num: 2
  player_num_per_team: 2
  step_mul: 8
  map_width: 64
  map_height: 64
  frame_limit: 3600
  manager_settings:
    food_manager:
        num_init: 260
        num_min: 260
        num_max: 300
    thorns_manager:
        num_init: 3
        num_min: 3
        num_max: 4
    player_manager:
        ball_settings:
            score_init: 13000
  playback_settings:
    playback_type: 'by_frame'
    by_frame:
      save_frame: False
collect:
  env_num: 32
evaluate:
  env_num: 5
  stop_value: 10000000000
  eval_episodes_num: 1
  eval_freq: 10000
agent:
  use_cuda: True
  pipeline: *pipeline
  rollout_nstep: 32 # train freq
  update_per_collect: 8 # gradient_steps
  load_checkpoint_path: ''
  print_collect_result: True
  print_eval_result: True
  target_update_interval: 1
  learning_starts: 0
  reward_type: 'log_reward'
  n_step: 1
  tau: 0.001 # soft update para
  action_num: 27
  features:
    max_ball_num: 80
    max_food_num: 256
    max_spore_num: 64
    direction_num: 12
    spatial_x: 64
    spatial_y: 64
  loss_parameters:
    gamma: 0.99
  replay_buffer:
    max_buffer_size: 40000
    batch_size: 512
  eps_greedy:
    type: 'frame'
    exploration_initial_eps: 1
    exploration_frames: 100000
    exploration_final_eps: 0.05
  optimizer:
    type: 'adam' # chosen from ['adam','rmsprop']
    learning_rate: 0.0003
    weight_decay: 0.0 # adam default 0
  grad_clip:
    type: 'clip_norm'
    threshold: 10
    norm_type: 2
learner:
  n_timesteps: 100000000
  log_show_freq: 100
  save_checkpoint_freq: 100000