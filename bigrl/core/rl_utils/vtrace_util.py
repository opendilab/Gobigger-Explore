import collections

import torch

# VTraceReturns = collections.namedtuple('VTraceReturns', ['vs', 'pg_advantages'])
VTraceReturns = collections.namedtuple('VTraceReturns', 'vs pg_advantages')


def vtrace_from_importance_weights(
        log_rhos, discounts, rewards, values, bootstrap_value,
        clip_rho_threshold=1.0, clip_pg_rho_threshold=1.0):
    r"""V-trace from log importance weights.
    Calculates V-trace actor critic targets as described in
    "IMPALA: Scalable Distributed Deep-RL with
    Importance Weighted Actor-Learner Architectures"
    by Espeholt, Soyer, Munos et al.
    In the notation used throughout documentation and comments, T refers to the
    time dimension ranging from 0 to T-1. B refers to the batch size and
    NUM_ACTIONS refers to the number of actions. This code also supports the
    case where all tensors have the same number of additional dimensions, e.g.,
    `rewards` is `[T, B, C]`, `values` is `[T, B, C]`, `bootstrap_value`
    is `[B, C]`.
    Args:
      log_rhos: A float32 tensor of shape `[T, B]` representing the
        log importance sampling weights, i.e.
        log(target_policy(a) / behaviour_policy(a)). V-trace performs operations
        on rhos in log-space for numerical stability.
      discounts: A float32 tensor of shape `[T, B]` with discounts encountered
        when following the behaviour policy.
      rewards: A float32 tensor of shape `[T, B]` containing rewards generated by
        following the behaviour policy.
      values: A float32 tensor of shape `[T, B]` with the value function estimates
        wrt. the target policy.
      bootstrap_value: A float32 of shape `[B]` with the value function estimate
        at time T.
      clip_rho_threshold: A scalar float32 tensor with the clipping threshold for
        importance weights (rho) when calculating the baseline targets (vs).
        rho^bar in the paper. If None, no clipping is applied.
      clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold
        on rho_s in \rho_s \delta log \pi(a|x) (r + \gamma v_{s+1} - V(x_s)). If
        None, no clipping is applied.
    Returns:
      A VTraceReturns namedtuple (vs, pg_advantages) where:
        vs: A float32 tensor of shape `[T, B]`. Can be used as target to
          train a baseline (V(x_t) - vs_t)^2.
        pg_advantages: A float32 tensor of shape `[T, B]`. Can be used as the
          advantage in the calculation of policy gradients.
    """
    # compute importance sampling weight
    rhos = torch.exp(log_rhos)
    clipped_rhos = torch.clamp(rhos, max=clip_rho_threshold)
    clipped_pg_rhos = torch.clamp(rhos, max=clip_pg_rho_threshold)
    cs = torch.clamp(rhos, max=1.0)

    # compute vtrace return
    # Append bootstrapped value to get [v1, ..., v_t+1]
    values_t_plus_1 = torch.cat([values[1:], bootstrap_value.unsqueeze(dim=0)], dim=0)
    deltas = clipped_rhos * (rewards + discounts * values_t_plus_1 - values)

    vs = values.clone()
    boostrap_term = 0
    for t in reversed(range(vs.shape[0])):
        boostrap_term = deltas[t] + discounts[t] * cs[t] * boostrap_term
        vs[t] += boostrap_term
    # compute pg advantage

    # Add V(x_s) to get v_s.
    vs_t_plus_1 = torch.cat((vs[1:], bootstrap_value.unsqueeze(dim=0)), dim=0)

    # Advantage for policy gradient.
    pg_advantages = (
            clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))
    return VTraceReturns(vs=vs,
                         pg_advantages=pg_advantages)
