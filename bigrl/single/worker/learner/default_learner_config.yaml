common:
  experiment_name: test
communication:
  league_ip: '127.0.0.1' # IMPORTANT, run coordinator on the node you specified
  league_port: 1234
  learner_send_train_info_freq: 400
  learner_send_model_freq: 4
  actor_model_update_interval: 3 # seconds
  send_data_num_workers: 1
  send_data_queue_size: 20
env:
  name: 'cartpole'
agent:
  player_id: MP0
  unroll_len : 5
learner:
  use_cuda: True
  pipeline: 'default'
  optimizer:
    type: 'adam' # chosen from ['adam','rmsprop']
    learning_rate: 0.0005
    weight_decay: 0.0
#    eps: 1e-4
#    decay: 0.99         # used in 'adam' 0.9 &'rmsprop' 0.99
#    momentum: 0         # used in 'adam' 0.999 &'rmsprop' 0
  grad_clip:
    type: 'clip_norm'
    threshold: 40
    norm_type: 2
  value_weight: 0.5
  entropy_weight: 0.01
  gamma: 0.99
  load_path: ''
  default_value_pretrain_iters: -1
  remain_value_pretrain_iters: -1
  data:
    batch_size: 32
    worker_num: 32
    max_buffer_size: 1000
    min_sample_size: 200
    start_sample_size: 2000 # store enough data before sampling, this should be larger than size, else, it will actuall be size
    max_use: 1 # max_use == 2 means data can be used at most two times
#    max_message_length: 4000000
  log_show_freq: 100
  save_checkpoint_freq: 200